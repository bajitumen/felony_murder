{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--ignore-certificate-errors')\n",
    "chrome_options.add_argument('--incognito')\n",
    "service = Service(\"/Users/tumendemberelshalkhaan/Downloads/chromedriver\")\n",
    "driver = webdriver.Chrome(service=service, options = chrome_options)\n",
    "driver.get('https://nysdoccslookup.doccs.ny.gov/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny = pd.DataFrame(columns=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('/Users/tumendemberelshalkhaan/Desktop/NY_DOC.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    ids = [item for row in reader for item in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(\"id\", \"lst\").send_keys('a')\n",
    "driver.find_element(\"xpath\", \"//button[contains(text(), 'Search')]\").click() \n",
    "\n",
    "time.sleep(2)   \n",
    "while True:\n",
    "    try:\n",
    "        next = driver.find_element(\"xpath\", \"//a[contains(text(), 'Next')]\")\n",
    "    except:\n",
    "        break\n",
    "    \n",
    "    table = driver.find_element('xpath', \"//table\")\n",
    "    rows = table.find_elements('xpath', '//tr')\n",
    "    \n",
    "    for row in rows:\n",
    "\n",
    "        cols = row.find_elements('xpath', \"//td\")\n",
    "        link = cols[0]\n",
    "        id = link.text\n",
    "        link.click()\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'lxml')\n",
    "        status = soup.find('h5', text = 'Custody Status:').findParent().find_next_sibling('div').get_text()\n",
    "\n",
    "        if 'IN CUSTODY' in status:\n",
    "\n",
    "            name = soup.find('h3', class_ = 'font-weight-bold').get_text()\n",
    "            demos = soup.find_all('div', class_ = 'col-sm-3')\n",
    "            race = demos[5].get_text()\n",
    "            dob = demos[6].get_text()\n",
    "            age = demos[7].get_text()\n",
    "\n",
    "            county = soup.find('h5', text = 'County of Commitment:').findParent().find_next_sibling('div').get_text()\n",
    "            fac = soup.find('h5', text = 'Housing / Releasing Facility:').findParent().find_next_sibling('div').get_text()\n",
    "            min_sen = soup.find('h5', text = 'Aggregate Minimum Sentence').findParent().find_next_sibling('div').get_text()\n",
    "            max_sen = soup.find('h5', text = 'Aggregate Maximum Sentence').findParent().find_next_sibling('div').get_text()\n",
    "            admit_date = soup.find('h5', text = 'Date Received (current):').findParent().find_next_sibling('div').get_text()\n",
    "\n",
    "            table = soup.find('table', class_ = 'table table-bordered text-nowrap').find('tbody')\n",
    "            offenses = []\n",
    "            for row in table.find_all('tr'):\n",
    "                data = [cell.get_text(strip=True) for cell in row.find_all('td')]\n",
    "                if data[0]:\n",
    "                    offenses.append(data[0])\n",
    "\n",
    "            rows = []           \n",
    "\n",
    "            for offense in offenses:\n",
    "                row = {\n",
    "                    'ID': id,\n",
    "                    'Name': name,\n",
    "                    'Date Of Birth': dob,\n",
    "                    'Race': race,\n",
    "                    'Facility': fac,\n",
    "                    'Offense Description': offense,\n",
    "                    'Admission Date': admit_date,\n",
    "                    'Minimum Sentence': min_sen,\n",
    "                    'Maximum Sentence': max_sen,\n",
    "                    'County': county\n",
    "                }\n",
    "                rows.append(row)\n",
    "\n",
    "            ny = pd.concat([ny, pd.DataFrame(rows)])\n",
    "        time.sleep(1)\n",
    "        driver.find_element('xpath', \"//a[contains(text(), '< Back to Search Results')]\").click()\n",
    "    \n",
    "        \n",
    "    try:\n",
    "        next.click()\n",
    "        \n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny.to_csv('/Users/tumendemberelshalkhaan/Desktop/New_York_Raw.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b1988c020eef046636d63474e1fd15d52ead3307ab1218b8c6d1bdc717b4bdae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
