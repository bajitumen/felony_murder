{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service(\"~/Downloads/chromedriver\")\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.get('https://doc.sd.gov/adult/lookup/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = pd.DataFrame(columns=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_ids =[]\n",
    "i = 2\n",
    "while True:\n",
    "    xpath = f\"//a[@href=\\\"javascript:__doPostBack('grvList','Page${i}')\\\"]\"\n",
    "    try:\n",
    "        links  = driver.find_elements(\"xpath\", \"//*[contains(@id,'grvList_hypDocId_')]\")\n",
    "        for x in range(len(links)):\n",
    "            links = driver.find_elements(\"xpath\", \"//*[contains(@id,'grvList_hypDocId_')]\")\n",
    "            doc = links[x].text\n",
    "            links[x].click()\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'lxml')\n",
    "            status = soup.find('span', {'id':'lblCorrStat'}).get_text()\n",
    "            if status == 'Active Inmate':\n",
    "                name = soup.find('span', {'id':'lblMainName'}).get_text()\n",
    "                age = soup.find('span', {'id':'lblAge'}).get_text()\n",
    "                id = soup.find('span', {'id':'lblDocId'}).get_text()\n",
    "                race = soup.find('span', {'id':'lblRace'}).get_text()\n",
    "                fac = soup.find('span', {'id':'lblCurrLoc'}).get_text()\n",
    "                sendate_elements = soup.find_all('td', {'class': 'sentdate'})\n",
    "                county_elements = soup.find_all('td', {'class': 'sentco'})\n",
    "                offense_elements = soup.find_all('td', {'class': 'sentcrime'})\n",
    "                sentence_elements = soup.find_all('td', {'class': 'sentlen'})\n",
    "\n",
    "                sendates = []\n",
    "                for sendate in sendate_elements:\n",
    "                    sendates.append(sendate.get_text())\n",
    "\n",
    "                offenses = []\n",
    "                for offense in offense_elements:\n",
    "                    offenses.append(offense.get_text())\n",
    "\n",
    "                counties = []\n",
    "                for county in county_elements:\n",
    "                    counties.append(county.get_text())\n",
    "\n",
    "                sentences = []\n",
    "                for sentence in sentence_elements:\n",
    "                    sentences.append(sentence.get_text())\n",
    "                \n",
    "                crimes = [list(pair) for pair in zip(offenses, sentences, sendates, counties)]\n",
    "                rows = []\n",
    "                for pair in crimes:\n",
    "                    row = {\n",
    "                        'ID': id,\n",
    "                        'Name': name,\n",
    "                        'Race': race,\n",
    "                        'Facility': fac,\n",
    "                        'Offense Description': pair[0],\n",
    "                        'Sentence Length': pair[1],\n",
    "                        'Sentence Date': pair[2],\n",
    "                        'County': pair[3]\n",
    "                    }\n",
    "                    rows.append(row)\n",
    "                    \n",
    "                sd = pd.concat([sd, pd.DataFrame(rows)])\n",
    "            driver.back()\n",
    "        driver.find_element('xpath', xpath).click()\n",
    "        i += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {doc}. Error: {e}\")\n",
    "        failed_ids.append(doc)\n",
    "        try:\n",
    "            driver.find_element('xpath', \"//a[contains(text(), '< Back to Search Results')]\").click()\n",
    "        except:\n",
    "            driver.back()\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.to_csv('/Users/tumendemberelshalkhaan/Desktop/South_Dakota_Raw.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b1988c020eef046636d63474e1fd15d52ead3307ab1218b8c6d1bdc717b4bdae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
